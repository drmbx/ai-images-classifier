"""
PyTorch Lightning –º–æ–¥—É–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ò–ò-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
"""

import torch
import torch.nn as nn
from torchmetrics import Accuracy, Precision, Recall, F1Score
import lightning as L
from ..model.model import AIImageClassifier


class AIImageClassifierModule(L.LightningModule):
    """
    Lightning –º–æ–¥—É–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ò–ò-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    """

    def __init__(
        self,
        backbone_name: str = "efficientnet_b0",
        num_classes: int = 2,
        dropout: float = 0.5,
        learning_rate: float = 1e-4,
        weight_decay: float = 1e-5,
        pretrained: bool = True,
        freeze_backbone: bool = False,
    ):
        """
        Args:
            backbone_name: –ù–∞–∑–≤–∞–Ω–∏–µ backbone –º–æ–¥–µ–ª–∏
            num_classes: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
            dropout: Dropout –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
            learning_rate: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
            weight_decay: Weight decay –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
            pretrained: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≤–µ—Å–∞
            freeze_backbone: –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞—Ç—å –ª–∏ –≤–µ—Å–∞ backbone (–Ω–∞–≤—Å–µ–≥–¥–∞)
        """
        super().__init__()
        self.save_hyperparameters()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        self.model = AIImageClassifier(
            backbone_name=backbone_name,
            num_classes=num_classes,
            dropout=dropout,
            pretrained=pretrained,
            freeze_backbone=freeze_backbone,
        )

        # Loss —Ñ—É–Ω–∫—Ü–∏—è: –∏—Å–ø–æ–ª—å–∑—É–µ–º CrossEntropyLoss –¥–ª—è –≤—Å–µ—Ö —Å–ª—É—á–∞–µ–≤
        self.criterion = nn.CrossEntropyLoss()

        # –ú–µ—Ç—Ä–∏–∫–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∂–∏–º multiclass –¥–∞–∂–µ –¥–ª—è 2 –∫–ª–∞—Å—Å–æ–≤,
        # —á—Ç–æ–±—ã —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ 0/1)
        self.train_accuracy = Accuracy(task="multiclass", num_classes=num_classes)
        self.val_accuracy = Accuracy(task="multiclass", num_classes=num_classes)
        self.test_accuracy = Accuracy(task="multiclass", num_classes=num_classes)

        self.val_precision = Precision(task="multiclass", num_classes=num_classes)
        self.val_recall = Recall(task="multiclass", num_classes=num_classes)
        self.val_f1 = F1Score(task="multiclass", num_classes=num_classes)

        self.learning_rate = learning_rate
        self.weight_decay = weight_decay

    def forward(self, x):
        """Forward pass"""
        return self.model(x)

    def training_step(self, batch, batch_idx):
        """–®–∞–≥ –æ–±—É—á–µ–Ω–∏—è"""
        images, labels = batch

        # Forward pass
        logits = self(images)

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ loss –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (–æ–±—â–∏–π —Å–ª—É—á–∞–π multiclass, –≤ —Ç–æ–º —á–∏—Å–ª–µ 2 –∫–ª–∞—Å—Å–∞)
        loss = self.criterion(logits, labels)
        preds = torch.argmax(logits, dim=1)

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        acc = self.train_accuracy(preds, labels)

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.log("train_loss", loss, on_step=True, on_epoch=True, prog_bar=True)
        self.log("train_acc", acc, on_step=True, on_epoch=True, prog_bar=True)

        # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç—É—Å –∑–∞–º–æ—Ä–æ–∑–∫–∏ —Ç–æ–ª—å–∫–æ –Ω–∞ –ø–µ—Ä–≤–æ–º —à–∞–≥–µ
        if self.current_epoch == 0 and batch_idx == 0:
            backbone_status = 0.0 if self.model.freeze_backbone else 1.0
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º self.log() –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –ª—é–±—ã–º –ª–æ–≥–≥–µ—Ä–æ–º
            self.log("backbone_status", backbone_status, on_step=False, on_epoch=False)
            if self.model.freeze_backbone:
                print("üßä Backbone –∑–∞–º–æ—Ä–æ–∂–µ–Ω - –æ–±—É—á–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä")
            else:
                print("üî• Backbone —Ä–∞–∑–º–æ—Ä–æ–∂–µ–Ω - –æ–±—É—á–∞–µ—Ç—Å—è –≤—Å—è –º–æ–¥–µ–ª—å")

        return loss

    def validation_step(self, batch, batch_idx):
        """–®–∞–≥ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
        images, labels = batch

        # Forward pass
        logits = self(images)

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ loss –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        loss = self.criterion(logits, labels)
        preds = torch.argmax(logits, dim=1)

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        self.val_accuracy(preds, labels)
        self.val_precision(preds, labels)
        self.val_recall(preds, labels)
        self.val_f1(preds, labels)

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.log("val_loss", loss, on_step=False, on_epoch=True, prog_bar=True)
        self.log(
            "val_acc", self.val_accuracy, on_step=False, on_epoch=True, prog_bar=True
        )
        self.log("val_precision", self.val_precision, on_step=False, on_epoch=True)
        self.log("val_recall", self.val_recall, on_step=False, on_epoch=True)
        self.log("val_f1", self.val_f1, on_step=False, on_epoch=True)

        return loss

    def test_step(self, batch, batch_idx):
        """–®–∞–≥ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        images, labels = batch

        # Forward pass
        logits = self(images)

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ loss –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        loss = self.criterion(logits, labels)
        preds = torch.argmax(logits, dim=1)

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        self.test_accuracy(preds, labels)

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.log("test_loss", loss, on_step=False, on_epoch=True)
        self.log("test_acc", self.test_accuracy, on_step=False, on_epoch=True)

        return loss

    def configure_optimizers(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ - –æ–±—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ —Ä–∞–∑–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã"""

        # –°–æ–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É—é—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        trainable_params = []
        frozen_params = []

        for name, param in self.model.named_parameters():
            if param.requires_grad:
                trainable_params.append(param)
            else:
                frozen_params.append(param)

        # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        print("üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏:")
        print(
            f"   –í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in self.model.parameters()):,}"
        )
        print(f"   –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in trainable_params):,}")
        print(f"   –ó–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in frozen_params):,}")
        print(
            f"   –û–±—É—á–∞–µ—Ç—Å—è: {sum(p.numel() for p in trainable_params) / sum(p.numel() for p in self.model.parameters()) * 100:.1f}% –º–æ–¥–µ–ª–∏"
        )

        # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        optimizer = torch.optim.AdamW(
            trainable_params, lr=self.learning_rate, weight_decay=self.weight_decay
        )

        # Learning rate scheduler
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode="min",
            factor=0.5,
            patience=5,
        )

        return {
            "optimizer": optimizer,
            "lr_scheduler": {"scheduler": scheduler, "monitor": "val_loss"},
        }
